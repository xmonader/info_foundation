# Datacenter Myths

![Logo](img/myth-big-datacenters-are-efficient.jpg?raw=true "Logo")
## Hyperscale Realities

### Myth 1: Big data centers are more efficient than small ones.

There are advantages in building large scale data centers, like economies of scale and shear bulk buying power compared to small ones, but these are not as big as people think. The average cost per rack in a hyper scale data center is $20-35K USD including all energy and safety systems. The cost of the hardware per rack is around the $200-300K USD mark.

What is often forgotten is that anything that is of enormous scale and therefore highly concentrated is complex and has a specific set of problems to deal with. Resource requirements for example, such as investment, operational costs, knowledge and people.

In reality keeping things simple and small is much more cost effective than a big complex environment.

### Myth 2: Big data centers can be made green.

The carbon footprint of a big datacenter is enormous. To improve the PUE (Power Usage Effectiveness) most data center farmers have adopted wind, hydro and/or solar power technologies, which indeed helps drop their PUE by an estimated 20%.

This 20% looks great on paper and in the farmers' corporate social responsibility reports. It is mostly, however, just an improvement on the cooling technology, not the actual energy consumed by the equipment that runs in their data centers (servers, storage chassis, physical disks, etc) and this equipment is what makes up the 100% of the carbon footprint. PUE only refers to overhead power consumption, i.e. cooling the facility, opening and closing doors, maintaining power security systems, etc.

So real improvement lies in deploying technologies that actually consume less power to deliver the actual Internet IT capacity to run workloads, real CPU chassis, physical disks and storage cabinets. Improving on how hardware is being more effectively used can have an impact of 1000% and lead to 10 times more power efficiency.

### Myth 3: Redundant systems have a better uptime.

A lot of us believe that systems need redundancy mechanisms to improve their operational uptime and reliability. To be honest if you look at it (through IT tainted glasses) for a moment this seems to make a lot of sense. But what if we translate this to the non-IT world?

To make a car more reliable we add redundancy (as we do in IT). So for the risk of having a puncture we add one extra tire for all the tires we use continuously. This adds 4 extra tires to the car. Then a decision needs to be made: Do we put those tires in a structure where they are always running along the primary tires or do we chose not to have them "on line" all the time, wearing and tearing in the same way as the primary tires?

Building such a system would take a large number of engineers come up with a solution, and would change cars as we know them. Would it not make more sense to think outside the box and solve the root of the problem by making tires un-deflatable?

The IT industry has gone overboard  with the concept of redundancy, having forgotten to look  at the root cause issues. This has spawned a whole new industry of itself, which has a financial interest in creating complicated and expensive redundant system.

### Myth 4: Big companies know how to optimize better.

Big companies with a certain track record will know better how to optimize, they have more peopleâ€¦'''

At first glance this sounds logical, but if we look at the IT landscape today 90%+ of innovation in IT is done by small startups. The big legacy IT companies have a heritage they hardly can overcome. They are locked in old infrastructure designs, and over time to building outside of that infrastructure is to put the breaks on their business.

Real innovation dies a slow death and gives way to fix the symptom rather than the problem pain-killer approach.
